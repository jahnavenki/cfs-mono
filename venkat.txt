#
# This file contains the rewrite rules, and can be customized.
#
# By default, it includes just the rewrite rules. You can
# add rewrite rules to this file but you should still include
# the default rewrite rules.

Include conf.d/rewrites/default_rewrite.rules

# rewrite for root redirect
RewriteRule ^/?$ /content/${CONTENT_FOLDER_NAME}/au/en/home.html [QSA,PT,L]

#Content redirects for shortened URLs in cfs-origination
RewriteRule ^/join/(.*)$ /content/cfs-origination/join/$1 [QSA,PT,L]

RewriteRule ^/home.html$ / [QSA,L,R=301]
RewriteRule ^/home$ / [QSA,L,R=301]

RewriteCond %{REQUEST_URI} !^/apps
RewriteCond %{REQUEST_URI} !^/bin
RewriteCond %{REQUEST_URI} !^/content
RewriteCond %{REQUEST_URI} !^/etc
RewriteCond %{REQUEST_URI} !^/home
RewriteCond %{REQUEST_URI} !^/libs
RewriteCond %{REQUEST_URI} !^/saml_login
RewriteCond %{REQUEST_URI} !^/system
RewriteCond %{REQUEST_URI} !^/tmp
RewriteCond %{REQUEST_URI} !^/var
RewriteCond %{REQUEST_URI} (.html|.jpe?g|.png|.svg|.model.json)$
RewriteRule ^/(.*)$ /content/${CONTENT_FOLDER_NAME}/au/en/$1 [QSA,PT,L]

RewriteCond %{REQUEST_URI} !^/content/cfs-winged/au/en/error-page.html$
RewriteRule ^/content/cfs-winged/au/en/(.*)  /$1 [QSA,L,R=301]

RewriteCond %{REQUEST_URI} !^/content/cfs-winged/au/en/error-page.html$
RewriteRule ^/content/cfs-origination/join/(.*) /join/$1 [QSA,L,R=301]


#Robots TXT
RewriteRule /robots.txt /bin/cfs/robots.txt?1=1 [PT,L]

#Sitemap
RewriteRule /sitemap.xml /content/cfs-winged/au/en.sitemap.xml?1=1 [PT,L]
RewriteRule ^/next/(.+)$ /$1 [QSA,L,R=302]

https://preview-p110482-e1080534.adobeaemcloud.com/personal/tools-resources/download-cfs-edge-app-now.html
https://preview-p110482-e1080534.adobeaemcloud.com/join-now.html?adobe_mc=MCMID%3D26344227651841094750042235830662926697%7CMCORGID%3DAB765EB55C544D790A495CFA%2540AdobeOrg%7CTS%3D1710125934.html


# Block preview URLs
RewriteCond %{REQUEST_URI} ^/preview [NC]
RewriteRule ^(.*)$ - [F,L]

# Block preview query parameters
RewriteCond %{QUERY_STRING} ^.*adobe_mc.*$ [NC]
RewriteRule ^(.*)$ - [F,L]

User-agent: *
Disallow: /preview
Disallow: /*adobe_mc=*






To effectively prevent the preview URLs from appearing in Google search results, it’s generally recommended to implement measures in both the robots.txt file and the dispatcher rules. Here’s why:

1. robots.txt File
Purpose: The robots.txt file is used to instruct search engine crawlers which parts of your website they are allowed or disallowed from crawling.
Implementation: By adding Disallow: /preview and other relevant paths to the robots.txt file, you prevent search engines from crawling those URLs altogether.
2. Dispatcher Rules
Purpose: Dispatcher rules work at the web server level to manage incoming requests, redirect URLs, and enforce access controls.
Implementation: Adding rewrite rules to block or redirect preview URLs helps ensure that they don’t get served to users or crawlers in the first place.
Why Both Are Important
Layered Defense: Using both methods creates a layered approach to prevent unwanted URLs from being indexed. If one method fails (e.g., a crawler ignoring robots.txt), the other can still act as a barrier.
Quick Response: If you implement rewrite rules in the dispatcher to block requests immediately, it prevents even the possibility of preview URLs being served, while the robots.txt acts as a secondary measure for compliance.
Summary
While it might be possible to rely on one method, using both the robots.txt file and dispatcher rules provides a more robust solution for blocking preview URLs from being indexed by search engines. This dual approach increases your chances of effectively managing your site's visibility in search results.
..........................
When it comes to handling URLs that have already been indexed by Google (like your preview URLs), implementing rules in your robots.txt file and dispatcher will not automatically remove these URLs from Google's index. However, they can help in the following ways:

1. Prevent Future Crawling
robots.txt: By adding rules to your robots.txt file to disallow the preview URLs (e.g., Disallow: /preview), you instruct Googlebot not to crawl these URLs anymore.
Dispatcher Rules: By redirecting or blocking these URLs at the web server level, you ensure that users and crawlers won't access the preview pages anymore.
2. Removing Indexed URLs
To remove already indexed URLs from Google's search results, you will need to take additional steps:

Google Search Console: Use the Removals tool in Google Search Console to request the removal of specific URLs. This tool allows you to temporarily hide URLs from Google search results. After a period, Google will stop showing those URLs, and they may be re-crawled based on your robots.txt rules.

404 or 410 Status Codes: If you return a 404 (Not Found) or 410 (Gone) status code for the preview URLs, Googlebot will eventually remove these URLs from its index. However, you should ensure these status codes are properly set up in your dispatcher rules to reflect the change.

Content Updates: If you can update the content at those URLs to indicate that the pages have been moved or are no longer available, this can also help in guiding Googlebot to remove them from the index.

3. Crawling Frequency
Keep in mind that Google may not immediately re-crawl your site after you make changes to the robots.txt file or dispatcher rules. The time it takes for Google to reflect these changes in its index can vary.
Summary
Implementing the changes in your robots.txt file and dispatcher rules will prevent future crawling of those URLs, but for already indexed URLs, you will need to actively request their removal through Google Search Console or implement proper server responses to guide Googlebot.
